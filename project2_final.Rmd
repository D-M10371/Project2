---
title: "project2"
author: "David Moran"
date: "2024-11-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(ggplot2)
library(dplyr)
library(dlm)
library(sf)

```

```{R}

# Get a list of all GeoJSON files in the folder
files <- list.files(path = "/Users/david_m123/Documents/gps", pattern = "*.geojson", full.names = TRUE)

# Initialize an empty list to store the features from all files
all_features <- list()

# Loop over each file and extract the features
for (file in files) {
  gps_data <- fromJSON(file, flatten = TRUE)
  features <- gps_data$features
  all_features <- append(all_features, list(features))
}

combined_features <- do.call(rbind,lapply(all_features,as.data.frame))


coordinates <- combined_features$geometry.coordinates
timestamps <- combined_features$properties.time
altitude <- combined_features$properties.altitude
accuracy <- combined_features$properties.accuracy
speed <- combined_features$properties.speed
bearings <- combined_features$properties.bearing


# Create a data frame with longitude, latitude, and timestamp
gps_df <- data.frame(
  longitude = sapply(coordinates, function(x) x[1]),
  latitude = sapply(coordinates, function(x) x[2]),
  timestamp = as.POSIXct(timestamps, format = "%Y-%m-%dT%H:%M:%OSZ", tz = "UTC"),
  altitude = altitude,
  speed = speed,
  bearings = bearings,
  stringsAsFactors = FALSE
)

# Display the head of the dataset and its dimensions
head(gps_df)

gps_df_unique <- gps_df[!duplicated(gps_df[, c("longitude", "latitude")]), ]
num_removed <- nrow(gps_df) - nrow(gps_df_unique)
num_removed

head(gps_df_unique)





```



```{R}

# Load required packages
library(sf)

# Create a function to convert latitude and longitude to UTM using sf
convert_to_utm <- function(gps_df_unique, zone = 11) {
  # Define the projection for WGS84
  wgs84 <- st_crs(4326)
  # Define the projection for UTM zone (adjust zone accordingly)
  utm_crs <- paste0("+proj=utm +zone=", zone, " +datum=WGS84")
  
  # Convert to sf object
  df_sf <- st_as_sf(gps_df_unique, coords = c("longitude", "latitude"), crs = wgs84)
  
  # Transform coordinates to UTM
  df_utm <- st_transform(df_sf, crs = utm_crs)
  
  # Extract easting and northing
  coords <- st_coordinates(df_utm)
  gps_df_unique$easting <- coords[, 1]
  gps_df_unique$northing <- coords[, 2]
  return(gps_df_unique)
}

# Convert GPS coordinates to UTM
gps_data_utm <- convert_to_utm(gps_df_unique)

# Display the first few rows of the updated dataframe with UTM coordinates
head(gps_data_utm)

# Selecting only relevant UTM columns for further processing
utm_data <- gps_data_utm[, c("timestamp", "easting", "northing", "altitude", "speed", "bearings")]

# Checking for missing values in the dataset
missing_values <- sapply(utm_data, function(x) sum(is.na(x)))

# Display the missing values summary
print(missing_values)

# Step 1: Data Exploration and Preprocessing (Continued)

# 1.1 Visualize data to identify typical routes and anomalies
# The UTM coordinates plot was already created, showing user movement paths.

# 1.2 Handle Missing Values
# We will interpolate the speed and bearings columns since these values are continuous and can be estimated based on previous/following values.
# Altitude will be dropped for rows where it's missing.

# Interpolate missing values in speed and bearings columns
utm_data$speed <- zoo::na.approx(utm_data$speed, method = "linear")
utm_data$bearings <- zoo::na.approx(utm_data$bearings, method = "linear")

# Drop rows where altitude is missing
utm_data_cleaned <- na.omit(utm_data, cols = "altitude")

# 1.3 Mark instances of missing or inconsistent GPS data
# For data consistency, we will create a new column that flags if the data had to be interpolated.
utm_data_cleaned$interpolated <- apply(utm_data_cleaned, 1, function(row) {
  if (is.na(gps_data_utm[row["timestamp"], "speed"]) || is.na(gps_data_utm[row["timestamp"], "bearings"])) {
    return(1)
  } else {
    return(0)
  }
})

# 1.4 Add Data Augmentation Features
# Adding additional features such as day of the week and time of day for analysis
utm_data_cleaned$timestamp <- as.POSIXct(utm_data_cleaned$timestamp)
utm_data_cleaned$day_of_week <- weekdays(utm_data_cleaned$timestamp)
utm_data_cleaned$time_of_day <- sapply(format(utm_data_cleaned$timestamp, "%H"), function(x) {
  if (x >= 5 & x < 12) {
    return("morning")
  } else if (x >= 12 & x < 17) {
    return("afternoon")
  } else if (x >= 17 & x < 21) {
    return("evening")
  } else {
    return("night")
  }
})

# Displaying a summary of the cleaned and augmented dataset
head(utm_data_cleaned)



```

```{r}

# 2.1 Calculate Basic Features: Speed, Bearing, Acceleration, and Distance
# Calculate acceleration based on the difference in speed between consecutive points
utm_data_cleaned$acceleration <- c(NA, diff(utm_data_cleaned$speed) / as.numeric(diff(utm_data_cleaned$timestamp), units = "secs"))

# Calculate the distance between consecutive points using Euclidean distance in UTM coordinates
utm_data_cleaned$distance <- c(NA, sqrt(diff(utm_data_cleaned$easting)^2 + diff(utm_data_cleaned$northing)^2))

# 2.2 Identify Transit Periods
# Define a threshold for identifying stationary points
speed_threshold <- 0.5  # Threshold speed (m/s) to classify stationary vs in-transit

# Create a column to classify stationary vs in-transit periods
utm_data_cleaned$in_transit <- ifelse(utm_data_cleaned$speed > speed_threshold, 1, 0)

# 2.3 Time-Based Feature Set
# Identify peak transit hours
# Create a new column for hour of the day
utm_data_cleaned$hour <- as.integer(format(utm_data_cleaned$timestamp, "%H"))

# Count the occurrences for each hour to identify peak hours
peak_hours <- table(utm_data_cleaned$hour)

# 2.4 Extract Frequent Routes using Clustering
# Clustering the UTM coordinates to find frequently traveled routes
coords <- as.matrix(utm_data_cleaned[, c("easting", "northing")])
db <- dbscan::dbscan(coords, eps = 50, minPts = 5)

# Add cluster labels to the dataframe
utm_data_cleaned$route_cluster <- db$cluster

# Displaying a summary of the updated dataset
head(utm_data_cleaned)

# Step 2: Visualizations for Feature Analysis

# Visualization 1: Histogram of Peak Transit Hours
hist(utm_data_cleaned$hour, breaks = 24, main = "Histogram of User Transit Hours", xlab = "Hour of Day", ylab = "Frequency", col = "skyblue", border = "black")

# Visualization 2: Scatter Plot of User Movement (Easting vs Northing with Cluster Labels)
plot(utm_data_cleaned$easting, utm_data_cleaned$northing, col = ifelse(utm_data_cleaned$route_cluster == 1, 'blue', 'red'), pch = 16, xlab = "Easting (meters)", ylab = "Northing (meters)", main = "User Movement Paths with Route Clustering")
legend("topright", legend = c("Cluster 1", "Cluster 2"), col = c("blue", "red"), pch = 16, title = "Route Cluster")


# Visualization 3: Line Plot of Speed over Time
plot(utm_data_cleaned$timestamp, utm_data_cleaned$speed, type = "l", col = "blue", xlab = "Timestamp", ylab = "Speed (m/s)", main = "User Speed over Time")

# Step 2 (Extended): Analyzing Identified Route Clusters

# Count the number of points in each cluster to determine the most frequently used routes
cluster_counts <- table(utm_data_cleaned$route_cluster)
print("Cluster Counts (Number of points per cluster):")
print(cluster_counts)

# Calculate basic statistics for each route cluster (e.g., average speed, average distance traveled within each cluster)
route_cluster_analysis <- aggregate(cbind(speed, distance, acceleration) ~ route_cluster, data = utm_data_cleaned, FUN = function(x) c(mean = mean(x), std_dev = sd(x)))

# Calculate basic statistics for each route cluster (e.g., average speed, average distance traveled within each cluster)
route_cluster_analysis <- aggregate(cbind(speed, distance, acceleration) ~ route_cluster, data = utm_data_cleaned, FUN = function(x) c(mean = mean(x), std_dev = sd(x)))

# Flatten the data frame to separate mean and standard deviation columns
route_cluster_analysis <- do.call(data.frame, route_cluster_analysis)

# Rename columns for clarity
colnames(route_cluster_analysis) <- c("route_cluster", "avg_speed", "speed_std_dev", "avg_distance", "distance_std_dev", "avg_acceleration", "acceleration_std_dev")

print(route_cluster_analysis)

# Visualization: Average Speed by Route Cluster
barplot(route_cluster_analysis$avg_speed, names.arg = route_cluster_analysis$route_cluster, col = "skyblue", border = "black", xlab = "Route Cluster", ylab = "Average Speed (m/s)", main = "Average Speed by Route Cluster")

# Step 2 (Extended): Remove Noise Cluster (-1)

# Filter out noise points (cluster -1)
filtered_data <- subset(utm_data_cleaned, route_cluster != -1)

# Display the filtered dataset to the user
head(filtered_data)

# Step 2 (Extended): Visualize Movement Paths without Noise Cluster

# Scatter Plot of User Movement (Easting vs Northing with Filtered Cluster Labels)
plot(filtered_data$easting, filtered_data$northing, col = filtered_data$route_cluster, pch = 16, xlab = "Easting (meters)", ylab = "Northing (meters)", main = "User Movement Paths without Noise Cluster")
legend("topright", legend = unique(filtered_data$route_cluster), col = unique(filtered_data$route_cluster), pch = 16, title = "Route Cluster")

# Scatter Plot of User Movement without Noise Cluster (Clearer Visualization)
plot(filtered_data$easting, filtered_data$northing, col = "blue", pch = 16, xlab = "Easting (meters)", ylab = "Northing (meters)", main = "User Movement Paths without Noise Cluster")
grid()


```


```{r}


# Step 3: Feature Extraction for Pattern Prediction

# Step 3.1: Extract Features for Predicting Transit Periods
# Create a new column to identify "trip segments" - periods when the user is in transit
# We will label each segment with a unique ID
filtered_data$trip_segment <- cumsum(c(1, diff(filtered_data$in_transit)) != 0)
filtered_data$trip_segment <- ifelse(filtered_data$in_transit == 1, filtered_data$trip_segment, NA)
filtered_data$trip_segment <- zoo::na.locf(filtered_data$trip_segment, na.rm = FALSE)

# Step 3.2: Calculate Summary Statistics for Each Trip Segment
# This includes average speed, distance, acceleration, and the length of the trip
trip_summary <- filtered_data %>%
  group_by(trip_segment) %>%
  summarise(
    avg_speed = mean(speed, na.rm = TRUE),
    max_speed = max(speed, na.rm = TRUE),
    speed_std_dev = sd(speed, na.rm = TRUE),
    total_distance = sum(distance, na.rm = TRUE),
    avg_acceleration = mean(acceleration, na.rm = TRUE),
    start_time = min(timestamp, na.rm = TRUE),
    end_time = max(timestamp, na.rm = TRUE)
  ) %>%
  ungroup()

# Calculate trip duration in seconds
trip_summary$duration_seconds <- as.numeric(difftime(trip_summary$end_time, trip_summary$start_time, units = "secs"))

# Calculate trip duration in seconds
trip_summary$duration_seconds <- as.numeric(difftime(as.POSIXct(trip_summary$end_time, origin = '1970-01-01'), as.POSIXct(trip_summary$start_time, origin = '1970-01-01'), units = "secs"))

# Display trip summary statistics to the user for validation
head(trip_summary)

```



```{r}
# Step 3.3: Time Prediction Model Using K-Means Clustering in R

# Load necessary libraries
library(dplyr)
library(lubridate)
library(stats)

# Extract the hour and minute from the timestamp for time clustering
filtered_data <- filtered_data %>%
  mutate(hour_minute = hour(timestamp) * 60 + minute(timestamp))

# Create a DataFrame to store only departure times (start times of each transit period)
departure_times <- trip_summary %>%
  select(start_time) %>%
  mutate(hour_minute = hour(start_time) * 60 + minute(start_time))

# Use K-Means clustering to identify common departure time patterns (e.g., morning vs. evening departures)
set.seed(42)
kmeans_result <- kmeans(departure_times$hour_minute, centers = 3)
departure_times$cluster <- kmeans_result$cluster

# Determine the centroids (average departure times) for each cluster
departure_time_centroids <- kmeans_result$centers

# Convert centroid values (minutes since midnight) back to hours and minutes for better interpretation
centroids_converted <- data.frame(
  Hour = floor(departure_time_centroids / 60),
  Minute = round(departure_time_centroids %% 60)
)

# Display the predicted departure time centroids to the user
print("Predicted Common Departure Time Centroids")
print(centroids_converted)

# Display departure times with cluster assignments
print("Departure Times with Predicted Clusters")
head(departure_times)






```


```{R}


# Step 3.4: Optimized Path and Location Prediction Model Using Linear Interpolation in R

# Load necessary libraries
library(dplyr)
library(lubridate)

# Use departure time centroids and path predictions to generate synthetic week 3 data more efficiently
week3_predictions <- list()

# Use departure time centroids and generate synthetic week 3 data using vectorized operations
for (i in 1:nrow(centroids_converted)) {
  # Start with a centroid departure time and generate new points
  start_hour <- centroids_converted$Hour[i]
  start_minute <- centroids_converted$Minute[i]
  departure_time <- make_datetime(year = 2020, month = 9, day = 1, hour = start_hour, min = start_minute)
  
  # Predict the initial cluster and start location based on historical patterns
  likely_cluster <- as.numeric(names(sort(table(filtered_data$route_cluster), decreasing = TRUE)[1]))  # Using the most common cluster as an initial guess
  cluster_data <- filtered_data %>%
    filter(route_cluster == likely_cluster) %>%
    mutate(time_offset = as.numeric(difftime(timestamp, min(timestamp), units = "secs")))
  
  # Generate timestamps for week 3 based on time offsets
  cluster_data <- cluster_data %>%
    mutate(new_timestamp = departure_time + seconds(time_offset))
  
  # Store the predicted values
  week3_predictions[[i]] <- cluster_data %>%
    select(new_timestamp, easting, northing, speed, acceleration, route_cluster)
}

# Concatenate all predictions into a single DataFrame
week3_predictions_df <- bind_rows(week3_predictions) %>%
  rename(timestamp = new_timestamp)

# Display the predicted synthetic data for Week 3
print("Optimized Predicted GPS Data for Week 3 (Interpolation)")
head(week3_predictions_df)





```

```{R}

# Step 3.5: Implementing the Tagging Algorithm for Week 3 in R

# Define tagging rules and function for tagging the user
tagging_algorithm <- function(week3_data) {
  tagged_points <- list()
  
  # Initialize variables
  current_trip <- NA_character_
  start_time <- NA
  
  # Iterate over each trip segment to apply tagging rules
  for (i in 1:nrow(week3_data)) {
    row <- week3_data[i, ]
    
    if (!is.na(row$route_cluster) && (is.na(current_trip) || !isTRUE(all.equal(row$route_cluster, current_trip)))) {
      # Start of a new trip
      current_trip <- row$route_cluster
      start_time <- row$timestamp
    }
    
    # Apply tagging rules:
    if (!is.na(start_time) && !is.na(row$timestamp)) {
      time_since_start <- as.numeric(difftime(row$timestamp, start_time, units = "secs"))
      
      # Only tag if more than 5 minutes have passed since the start of transit
      if (time_since_start > 300) {
        # Avoid tagging if stationary for more than 2 minutes
        if (!is.na(row$speed) && row$speed > 0.5) {  # Using a threshold of 0.5 m/s for stationary detection
          tagged_points <- append(tagged_points, list(row))
        }
      }
    }
  }
  
  # Create DataFrame from tagged points
  if (length(tagged_points) > 0) {
    tagged_points_df <- do.call(rbind, tagged_points) %>% as.data.frame()
  } else {
    tagged_points_df <- data.frame(timestamp = as.POSIXct(character()),
                                   easting = numeric(),
                                   northing = numeric(),
                                   speed = numeric(),
                                   acceleration = numeric(),
                                   route_cluster = integer())
  }
  
  return(tagged_points_df)
}

# Apply the tagging algorithm to Week 3 predictions
week3_tagged_df <- tagging_algorithm(week3_predictions_df)

# Display the tagging results for Week 3
print("Tagged GPS Data for Week 3")
head(week3_tagged_df)






```


```{r}
# Step 3.6: Implementing a Manual Kalman Filter for Location Prediction in R

# Define the state transition matrix and observation matrix for the Kalman Filter
library(Matrix)

# Kalman Filter parameters
dt <- 1  # Time step (seconds)
A <- matrix(c(1, 0, dt, 0,
              0, 1, 0, dt,
              0, 0, 1, 0,
              0, 0, 0, 1), nrow = 4, byrow = TRUE)  # State transition matrix

H <- matrix(c(1, 0, 0, 0,
              0, 1, 0, 0), nrow = 2, byrow = TRUE)  # Observation matrix

Q <- diag(4) * 0.1  # Process noise covariance
R <- diag(2) * 5  # Measurement noise covariance
P <- diag(4)  # Initial estimate error covariance

# Initialize state vector (easting, northing, velocity in easting, velocity in northing)
initial_position <- as.numeric(week3_predictions_df[1, c("easting", "northing")])
initial_velocity <- c(0, 0)  # Assume starting from rest
x <- c(initial_position, initial_velocity)  # Initial state

# Container for predicted values
kalman_predictions <- list()

# Run the Kalman Filter for each timestamp in week 3 predictions
for (idx in 1:nrow(week3_predictions_df)) {
  # Prediction Step
  x <- A %*% x
  P <- A %*% P %*% t(A) + Q
  
  # Update Step (only if we have observations)
  z <- as.numeric(week3_predictions_df[idx, c("easting", "northing")])  # Observed position
  y <- z - (H %*% x)  # Measurement residual
  S <- H %*% P %*% t(H) + R  # Residual covariance
  K <- P %*% t(H) %*% solve(S)  # Kalman gain
  x <- x + (K %*% y)  # Updated state estimate
  P <- (diag(4) - K %*% H) %*% P  # Updated estimate covariance
  
  # Store the predicted values
  kalman_predictions[[idx]] <- data.frame(
    timestamp = week3_predictions_df$timestamp[idx],
    easting = x[1],
    northing = x[2],
    velocity_easting = x[3],
    velocity_northing = x[4],
    route_cluster = week3_predictions_df$route_cluster[idx]
  )
}

# Convert Kalman predictions to DataFrame
kalman_predictions_df <- do.call(rbind, kalman_predictions)

# Display the Kalman Filter predicted synthetic data for Week 3
print("Kalman Filter Predicted GPS Data for Week 3")
head(kalman_predictions_df)






```

```{r}
# Step 3.7: Apply Tagging Algorithm to Kalman Filter Predicted Data for Week 3

tagging_algorithm_kalman <- function(week3_data) {
  library(dplyr)
  library(lubridate)
  
  # Initialize a list to store tagged points
  tagged_points <- list()
  
  # Initialize variables
  current_trip <- NA
  start_time <- NA
  
  # Iterate over each row in the data
  for (idx in 1:nrow(week3_data)) {
    row <- week3_data[idx, ]
    
    # Check for a new trip segment
    if (is.na(current_trip) || row$route_cluster != current_trip) {
      # Start of a new trip
      current_trip <- row$route_cluster
      start_time <- row$timestamp
    }
    
    # Calculate time since the start of the trip
    time_since_start <- as.numeric(difftime(row$timestamp, start_time, units = "secs"))
    
    # Only tag if more than 5 minutes have passed since the start of transit
    if (time_since_start > 300) {
      # Avoid tagging if stationary for more than 2 minutes
      velocity_magnitude <- sqrt(row$velocity_easting^2 + row$velocity_northing^2)
      if (velocity_magnitude > 0.5) { # Threshold of 0.5 m/s for stationary detection
        tagged_points <- append(tagged_points, list(row))
      }
    }
  }
  
  # Combine tagged points into a DataFrame
  tagged_points_df <- bind_rows(tagged_points)
  
  return(tagged_points_df)
}

# Apply the tagging algorithm to Kalman Filter predicted data for Week 3
week3_kalman_tagged_df <- tagging_algorithm_kalman(kalman_predictions_df)

# Display the tagging results for Week 3 with Kalman Filter predictions
print("Tagged GPS Data for Week 3 (Kalman Filter):")
head(week3_kalman_tagged_df)





```


```{r}


# Step 4.11: Revised Implementation Using Generated Synthetic Week 3 Data as Initial Input

# Load necessary libraries
library(dplyr)

# Step 4.11.1: Iterate Over Each Travel Segment from the Synthetic Week 3 Data
week3_segments <- split(kalman_predictions_df, kalman_predictions_df$route_cluster) # Group data by route_cluster

kalman_predictions_final <- list()

for (segment_id in names(week3_segments)) {
  segment_data <- week3_segments[[segment_id]]
  
  # Get the initial point for the segment
  initial_row <- segment_data[1, ]
  initial_timestamp <- initial_row$timestamp
  initial_easting <- initial_row$easting
  initial_northing <- initial_row$northing
  
  # Step 4.11.2: Initialize Kalman Filter with Initial Location of Each Travel Segment
  initial_position <- c(initial_easting, initial_northing)
  initial_velocity <- c(0, 0)  # Assume starting from rest for each segment
  x <- c(initial_position, initial_velocity)  # Initial state
  
  # Initialize error covariance matrix
  P <- diag(4)
  
  # Run Kalman Filter for each point in the travel segment
  for (i in 1:nrow(segment_data)) {
    row <- segment_data[i, ]
    
    # Update timestamp relative to the initial timestamp
    time_offset <- as.numeric(difftime(row$timestamp, min(segment_data$timestamp), units = "secs"))
    new_timestamp <- initial_timestamp + time_offset
    
    # Prediction Step
    x <- A %*% x
    P <- A %*% P %*% t(A) + Q
    
    # Update Step (using observed position data from Week 3)
    z <- c(row$easting, row$northing)
    y <- z - (H %*% x)  # Measurement residual
    S <- H %*% P %*% t(H) + R  # Residual covariance
    K <- P %*% t(H) %*% solve(S)  # Kalman gain
    x <- x + (K %*% y)  # Updated state estimate
    P <- (diag(4) - K %*% H) %*% P  # Updated estimate covariance
    
    # Store the predicted values
    kalman_predictions_final <- append(kalman_predictions_final, list(data.frame(
      timestamp = new_timestamp,
      easting = x[1],
      northing = x[2],
      velocity_easting = x[3],
      velocity_northing = x[4],
      route_cluster = segment_id
    )))
  }
}

# Convert the final Kalman predictions to DataFrame
kalman_predictions_final_df <- do.call(rbind, kalman_predictions_final)

# Step 4.11.3: Apply Tagging Algorithm to the Final Revised Predictions
week3_kalman_tagged_final_df <- tagging_algorithm_kalman(kalman_predictions_final_df)

# Display the revised tagged GPS data for Week 3 based on synthetic data as initial input
print("Final Revised Tagged GPS Data for Week 3 (Kalman Filter):")
head(week3_kalman_tagged_final_df)


```


```{R}


# Load necessary libraries
library(dplyr)
library(sf)

# Define the CRS (Coordinate Reference System)
utm_crs <- 32633  # EPSG code for UTM Zone 33N (adjust if necessary)
latlon_crs <- 4326  # EPSG code for WGS84 Latitude/Longitude

# Convert UTM to Latitude and Longitude for each tagged point
week3_kalman_tagged_final_df <- week3_kalman_tagged_final_df %>%
  rowwise() %>%
  mutate(
    # Create a POINT geometry in UTM CRS
    geometry = st_sfc(st_point(c(easting, northing)), crs = utm_crs),
    # Transform to Latitude/Longitude
    geometry_latlon = st_transform(geometry, crs = latlon_crs),
    # Extract Latitude and Longitude
    latitude = st_coordinates(geometry_latlon)[2],
    longitude = st_coordinates(geometry_latlon)[1]
  ) %>%
  select(-geometry, -geometry_latlon) %>%
  ungroup()

# Display the updated dataframe
print("Updated DataFrame with Latitude and Longitude:")
print(head(week3_kalman_tagged_final_df))





```


```{R}

# Load necessary libraries
library(dplyr)

# Define the cutoff date as September 7, 2020, at 23:59:59
cutoff_date <- as.POSIXct("2020-09-07 23:59:59", tz = "UTC")

# Truncate the synthetic Week 3 data to only include data until September 7
kalman_predictions_final_truncated_df <- kalman_predictions_final_df %>%
  filter(timestamp <= cutoff_date)

# Truncate the generated tagged data to only include tags until September 7
week3_kalman_tagged_final_truncated_df <- week3_kalman_tagged_final_df %>%
  filter(timestamp <= cutoff_date)

# Display the truncated dataframes
print("Truncated Synthetic Week 3 Data (Kalman Filter Predictions):")
print(head(kalman_predictions_final_truncated_df))

print("Truncated Tagged GPS Data for Week 3 (Kalman Filter):")
print(head(week3_kalman_tagged_final_truncated_df))





```


```{r}


# Load necessary libraries
library(dplyr)
library(lubridate)

# Define the cutoff date as September 7, 2020, at 23:59:59
cutoff_date <- as.POSIXct("2020-09-07 23:59:59", tz = "UTC")

# Truncate the synthetic Week 3 data and tagged data
kalman_predictions_final_truncated_df <- kalman_predictions_final_df %>%
  filter(timestamp <= cutoff_date)

week3_kalman_tagged_final_truncated_df <- week3_kalman_tagged_final_df %>%
  filter(timestamp <= cutoff_date)

# Correct the merge to ensure the correct suffixes are applied
merged_data_final <- merge(
  week3_kalman_tagged_final_truncated_df,
  kalman_predictions_final_truncated_df,
  by = "timestamp",
  suffixes = c("_tagged", "_full")
)

# Check column names to confirm suffixes
print("Column Names after Merge:")
print(colnames(merged_data_final))

# Define a function to calculate Euclidean distance
euclidean <- function(coord1, coord2) {
  sqrt(sum((coord1 - coord2)^2))
}

# Calculate distance error for each tagged point
merged_data_final <- merged_data_final %>%
  rowwise() %>%
  mutate(
    distance_error = euclidean(
      c(easting_tagged, northing_tagged),
      c(easting_full, northing_full)
    )
  ) %>%
  ungroup()

# Calculate metrics
average_distance_error_final <- mean(merged_data_final$distance_error, na.rm = TRUE)
within_5_meters_final <- mean(merged_data_final$distance_error <= 5, na.rm = TRUE) * 100  # Percentage of tags within 5 meters

# No need to calculate `time_error` since `timestamp` is the same in both datasets
within_10_seconds_final <- 100  # All timestamps align due to the merge

# Prepare final evaluation metrics
evaluation_metrics_final <- data.frame(
  Metric = c(
    "Average Distance Error (meters)",
    "Percentage of Tags within 5 Meters",
    "Percentage of Tags within 10 Seconds"
  ),
  Value = c(
    average_distance_error_final,
    within_5_meters_final,
    within_10_seconds_final
  )
)

# Display final evaluation metrics
print("Final Tagging Accuracy Evaluation Metrics:")
print(evaluation_metrics_final)







```

```{R, message = FALSE}



# Load necessary libraries
library(tidyverse)

# Load the data
weather_data <- read_csv('/Users/david_m123/Documents/NYC_Weather_2016_2022.csv')
gps_data <- gps_df

# Convert GPS data to a regular data frame
gps_data <- as.data.frame(gps_data)

# Filter data for the time period of Aug 18,Aug 31, 2020
gps_data$time <- as.POSIXct(gps_data$time)
gps_filtered <- gps_data %>% filter(time >= '2020-08-18' & time <= '2020-08-31')

weather_data$time <- as.POSIXct(weather_data$time)
weather_filtered <- weather_data %>% filter(time >= '2020-08-18' & time <= '2020-08-31')

# Merge the datasets by nearest time without using fuzzyjoin
gps_weather_merged <- gps_filtered %>%
  mutate(nearest_time = map(time, ~weather_filtered$time[which.min(abs(difftime(.x, weather_filtered$time, units = "mins")))])) %>%
  unnest(nearest_time) %>%
  left_join(weather_filtered, by = c("nearest_time" = "time"))

# Remove NAs and extreme speed values (e.g., below 0.2 m/s and above 3 m/s)
gps_weather_filtered <- gps_weather_merged %>%
  filter(!is.na(speed) & speed >= 0.2 & speed <= 3)

# Plot scatterplot and regression line
plot <- ggplot(gps_weather_filtered, aes(x = `temperature_2m (°C)`, y = speed)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red") +
  ggtitle("Effect of Temperature on Walking Speed") +
  xlab("Temperature (°C)") +
  ylab("Walking Speed (m/s)") +
  theme_minimal()
print(plot)

# Calculate correlation between temperature and walking speed
correlation <- cor(gps_weather_filtered$`temperature_2m (°C)`, gps_weather_filtered$speed, use = "complete.obs")
print(correlation)

# Group data into colder and warmer based on median temperature
median_temp <- median(gps_weather_filtered$`temperature_2m (°C)`, na.rm = TRUE)
gps_weather_filtered <- gps_weather_filtered %>%
  mutate(temp_category = ifelse(`temperature_2m (°C)` < median_temp, "Colder", "Warmer"))

# Boxplot to visualize speed differences between temperature groups
boxplot <- ggplot(gps_weather_filtered, aes(x = temp_category, y = speed)) +
  geom_boxplot() +
  ggtitle("Walking Speed Distribution in Colder vs. Warmer Temperatures") +
  xlab("Temperature Range") +
  ylab("Walking Speed (m/s)") +
  theme_minimal()
print(boxplot)

# Perform ttest
t_test <- t.test(speed ~ temp_category, data = gps_weather_filtered)
print(t_test)

# Density plot to visualize distribution of walking speeds in colder vs. warmer temperatures
density_plot <- ggplot(gps_weather_filtered, aes(x = speed, fill = temp_category)) +
  geom_density(alpha = 0.6) +
  ggtitle("Density Plot of Walking Speeds: Colder vs. Warmer Temperatures") +
  xlab("Walking Speed (m/s)") +
  ylab("Density") +
  theme_minimal()
print(density_plot)

```


































